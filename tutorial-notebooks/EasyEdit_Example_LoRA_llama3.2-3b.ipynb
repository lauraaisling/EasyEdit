{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "248097b8",
   "metadata": {},
   "source": [
    "# EasyEdit Example with **LoRA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753b8801",
   "metadata": {},
   "source": [
    "In this tutorial, we use `LoRA` to edit `llama-3.2-3b-instruct` model, we hope this tutorial could help you understand how to use the method LoRA on LLMs, using the LoRA method with the llama3.2-3b-instruct as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0a1701",
   "metadata": {},
   "source": [
    "## Model Editing\n",
    "\n",
    "Deployed models may still make unpredictable errors. For example, Large Language Models (LLMs) notoriously hallucinate, perpetuate bias, and factually decay, so we should be able to adjust specific behaviors of pre-trained models.\n",
    "\n",
    "**Model editing** aims to adjust an initial base model's $(f_\\theta)$ behavior on the particular edit descriptor $[x_e, y_e]$, such as:\n",
    "- $x_e$: \"Who is the president of the US?\n",
    "- $y_e$: \"Joe Biden.\"\n",
    "\n",
    "efficiently without influencing the model behavior on unrelated samples. The ultimate goal is to create an edited model$(f_\\thetaâ€™)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9717af3a",
   "metadata": {},
   "source": [
    "## ðŸ“‚ Data Preparation\n",
    "\n",
    "The datasets used can be found in [Google Drive Link](https://drive.google.com/file/d/1YtQvv4WvTa4rJyDYQR2J-uK8rnrt0kTA/view?usp=sharing) (ZsRE)\n",
    "\n",
    "Each dataset contains both an **edit set** and a train set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf48075",
   "metadata": {},
   "source": [
    "## Prepare the runtime environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6356ed23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/8t/fangjizhan/EasyEdit\n",
      "data\t    examples  multimodal_edit.py   run_wise_editing.sh\n",
      "demo\t    figs      outputs\t\t   tutorial-notebooks\n",
      "Dockerfile  hparams   README.md\t\t   tutorial.pdf\n",
      "easyeditor  LICENSE   requirements.txt\n",
      "edit.py     logs      run_wise_editing.py\n"
     ]
    }
   ],
   "source": [
    "## Clone Repo\n",
    "#!git clone https://github.com/zjunlp/EasyEdit\n",
    "%cd EasyEdit\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a104cd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install python3.9\n",
    "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 1\n",
    "!sudo update-alternatives --config python3\n",
    "!apt-get install python3-pip\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b039c94a",
   "metadata": {},
   "source": [
    "## Config Method  Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d553b513",
   "metadata": {},
   "source": [
    "```python\n",
    "alg_name: \"LoRA\"\n",
    "model_name: \"./hugging_cache/llama-3.2-3b-instruct\"\n",
    "device: 0\n",
    "\n",
    "lora_type: \"adalora\"\n",
    "layers: []\n",
    "num_steps: 70\n",
    "batch_size: 1\n",
    "max_length: 30\n",
    "lr: 5e-3\n",
    "weight_decay: 0\n",
    "kl_factor: 0\n",
    "rank: 8\n",
    "lora_alpha: 32\n",
    "lora_dropout: 0.1\n",
    "norm_constraint: false\n",
    "target_modules: [\"q_proj\", \"v_proj\"]  #[\"up_proj\", \"down_proj\"] #[\"q_proj\", \"v_proj\"]\n",
    "model_parallel: false\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9aef0a",
   "metadata": {},
   "source": [
    "## Import models & Run\n",
    "\n",
    "### Edit llama-3.2-3b-instruct on ZsRE with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c0a266f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/8t/xkw/EasyEdit\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2100450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyeditor import BaseEditor\n",
    "from easyeditor import LoRAHyperParams\n",
    "import json\n",
    "K = 3\n",
    "edit_data = json.load(open('./data/ZsRE/zsre_mend_edit.json', 'r', encoding='utf-8'))[:K]\n",
    "loc_data = json.load(open('./data/ZsRE/zsre_mend_train.json', 'r', encoding='utf-8'))[:K]\n",
    "loc_prompts = [edit_data_['loc'] + ' ' + edit_data_['loc_ans'] for edit_data_ in loc_data]\n",
    "\n",
    "prompts = [edit_data_['src'] for edit_data_ in edit_data]\n",
    "subject = [edit_data_['subject'] for edit_data_ in edit_data]\n",
    "rephrase_prompts = [edit_data_['rephrase'] for edit_data_ in edit_data]\n",
    "target_new = [edit_data_['alt'] for edit_data_ in edit_data]\n",
    "locality_prompts = [edit_data_['loc'] for edit_data_ in edit_data]\n",
    "locality_ans = [edit_data_['loc_ans'] for edit_data_ in edit_data]\n",
    "locality_inputs = {\n",
    "    'neighborhood':{\n",
    "        'prompt': locality_prompts,\n",
    "        'ground_truth': locality_ans\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a71267b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ Edit Data:0 ------------------------\n",
      "subject  :  IAAF Combined Events Challenge\n",
      "src  :  When was the inception of IAAF Combined Events Challenge?\n",
      "pred  :  2011\n",
      "rephrase  :  When was the IAAF Combined Events Challenge launched?\n",
      "alt  :  2006\n",
      "answers  :  ['1998']\n",
      "loc  :  nq question: what is the name of the last episode of spongebob\n",
      "loc_ans  :  The String\n",
      "cond  :  2011 >> 2006 || When was the inception of IAAF Combined Events Challenge?\n",
      "portability  :  {'Recalled Relation': '(IAAF Combined Events Challenge, event type, athletics)', 'New Question': 'What type of sports event is the IAAF Combined Events Challenge, which was established in 2006?', 'New Answer': 'Athletics'}\n",
      "\n",
      "------------------ Edit Data:1 ------------------------\n",
      "subject  :  Ramalinaceae\n",
      "src  :  Which family does Ramalinaceae belong to?\n",
      "pred  :  Ramalinales\n",
      "rephrase  :  What family are Ramalinaceae?\n",
      "alt  :  Lamiinae\n",
      "answers  :  ['Lecanorales']\n",
      "loc  :  nq question: types of skiing in the winter olympics 2018\n",
      "loc_ans  :  Downhill\n",
      "cond  :  Ramalinales >> Lamiinae || Which family does Ramalinaceae belong to?\n",
      "portability  :  {'Recalled Relation': '(Lamiinae, subfamily of, Cerambycidae)', 'New Question': 'Which family does Ramalinaceae now belong to after the reclassification?', 'New Answer': 'Cerambycidae'}\n",
      "\n",
      "------------------ Edit Data:2 ------------------------\n",
      "subject  :  Call the Doctor\n",
      "src  :  What artist created Call the Doctor?\n",
      "pred  :  Riders in the Sky\n",
      "rephrase  :  Which artist created Call the Doctor?\n",
      "alt  :  The X-Files\n",
      "answers  :  ['Sleater-Kinney']\n",
      "loc  :  nq question: who sang nice day for a white wedding\n",
      "loc_ans  :  Billy Idol\n",
      "cond  :  Riders in the Sky >> The X-Files || What artist created Call the Doctor?\n",
      "portability  :  {'Recalled Relation': '(The X-Files, lead actors, David Duchovny and Gillian Anderson)', 'New Question': 'Who were the lead actors in the series that inspired the creators of Call the Doctor?', 'New Answer': 'David Duchovny and Gillian Anderson'}\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(edit_data):\n",
    "    print(f\"\\n------------------ Edit Data:{i} ------------------------\")\n",
    "    for k,v in data.items():\n",
    "        print(k,\" : \", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ca0f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-28 19:26:43,969 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "10/28/2024 19:26:43 - INFO - easyeditor.editors.editor -   Instantiating model\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004863262176513672,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "303d6140cc4444a6bbce71d392ee2443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-28 19:26:45,830 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to left...\n",
      "10/28/2024 19:26:45 - INFO - easyeditor.editors.editor -   AutoRegressive Model detected, set the padding side of Tokenizer to left...\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.85it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,441,312 || all params: 3,216,191,192 || trainable%: 0.1070\n",
      "Executing LoRA algo for: [When was the inception of IAAF Combined Events Challenge?] -> [2006]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.8584980964660645\n",
      "Total loss 2.8584980964660645\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.6745290756225586\n",
      "Total loss 2.6745290756225586\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.172943115234375\n",
      "Total loss 2.172943115234375\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 1.1754391193389893\n",
      "Total loss 1.1754391193389893\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.5019904375076294\n",
      "Total loss 0.5019904375076294\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.4198444187641144\n",
      "Total loss 0.4198444187641144\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.4358944296836853\n",
      "Total loss 0.4358944296836853\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.3821360468864441\n",
      "Total loss 0.3821360468864441\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.3414689898490906\n",
      "Total loss 0.3414689898490906\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.31299424171447754\n",
      "Total loss 0.31299424171447754\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.27210503816604614\n",
      "Total loss 0.27210503816604614\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.22240114212036133\n",
      "Total loss 0.22240114212036133\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.19247768819332123\n",
      "Total loss 0.19247768819332123\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.18193426728248596\n",
      "Total loss 0.18193426728248596\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.16686154901981354\n",
      "Total loss 0.16686154901981354\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.14229559898376465\n",
      "Total loss 0.14229559898376465\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.12842515110969543\n",
      "Total loss 0.12842515110969543\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.11744388937950134\n",
      "Total loss 0.11744388937950134\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.10198739916086197\n",
      "Total loss 0.10198739916086197\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.0885382816195488\n",
      "Total loss 0.0885382816195488\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.0845072939991951\n",
      "Total loss 0.0845072939991951\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.08465169370174408\n",
      "Total loss 0.08465169370174408\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.0814703032374382\n",
      "Total loss 0.0814703032374382\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.07506706565618515\n",
      "Total loss 0.07506706565618515\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.07213615626096725\n",
      "Total loss 0.07213615626096725\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.07031697034835815\n",
      "Total loss 0.07031697034835815\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.0650370791554451\n",
      "Total loss 0.0650370791554451\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.05994001775979996\n",
      "Total loss 0.05994001775979996\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.06083984300494194\n",
      "Total loss 0.06083984300494194\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.05718659609556198\n",
      "Total loss 0.05718659609556198\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.05326802283525467\n",
      "Total loss 0.05326802283525467\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.05454859882593155\n",
      "Total loss 0.05454859882593155\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.05151856690645218\n",
      "Total loss 0.05151856690645218\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.049061086028814316\n",
      "Total loss 0.049061086028814316\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.048307184129953384\n",
      "Total loss 0.048307184129953384\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.04638677090406418\n",
      "Total loss 0.04638677090406418\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.044854287058115005\n",
      "Total loss 0.044854287058115005\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.04325030371546745\n",
      "Total loss 0.04325030371546745\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.04280770197510719\n",
      "Total loss 0.04280770197510719\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.04102016240358353\n",
      "Total loss 0.04102016240358353\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.03998815640807152\n",
      "Total loss 0.03998815640807152\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.04047747701406479\n",
      "Total loss 0.04047747701406479\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.040042780339717865\n",
      "Total loss 0.040042780339717865\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.038126494735479355\n",
      "Total loss 0.038126494735479355\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.03730529174208641\n",
      "Total loss 0.03730529174208641\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.038050856441259384\n",
      "Total loss 0.038050856441259384\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.03674157336354256\n",
      "Total loss 0.03674157336354256\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.038205355405807495\n",
      "Total loss 0.038205355405807495\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.03592976927757263\n",
      "Total loss 0.03592976927757263\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0384809784591198\n",
      "Total loss 0.0384809784591198\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.03653766214847565\n",
      "Total loss 0.03653766214847565\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.03636837750673294\n",
      "Total loss 0.03636837750673294\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.03678472712635994\n",
      "Total loss 0.03678472712635994\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.03544158115983009\n",
      "Total loss 0.03544158115983009\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.036348335444927216\n",
      "Total loss 0.036348335444927216\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.034861523658037186\n",
      "Total loss 0.034861523658037186\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.035893503576517105\n",
      "Total loss 0.035893503576517105\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03477837145328522\n",
      "Total loss 0.03477837145328522\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03639233112335205\n",
      "Total loss 0.03639233112335205\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.03459944203495979\n",
      "Total loss 0.03459944203495979\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03504746034741402\n",
      "Total loss 0.03504746034741402\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03469181805849075\n",
      "Total loss 0.03469181805849075\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0352729856967926\n",
      "Total loss 0.0352729856967926\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.034074295312166214\n",
      "Total loss 0.034074295312166214\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.035803597420454025\n",
      "Total loss 0.035803597420454025\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03549009934067726\n",
      "Total loss 0.03549009934067726\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.035192783921957016\n",
      "Total loss 0.035192783921957016\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03528492897748947\n",
      "Total loss 0.03528492897748947\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03481215611100197\n",
      "Total loss 0.03481215611100197\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.035084422677755356\n",
      "Total loss 0.035084422677755356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:13<00:27, 13.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing LoRA algo for: [Which family does Ramalinaceae belong to?] -> [Lamiinae]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 8.995731353759766\n",
      "Total loss 8.995731353759766\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.2292027473449707\n",
      "Total loss 1.2292027473449707\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.2446369081735611\n",
      "Total loss 0.2446369081735611\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.012890488840639591\n",
      "Total loss 0.012890488840639591\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.0021752684842795134\n",
      "Total loss 0.0021752684842795134\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.0009577595046721399\n",
      "Total loss 0.0009577595046721399\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.0012084963964298368\n",
      "Total loss 0.0012084963964298368\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.0011989891063421965\n",
      "Total loss 0.0011989891063421965\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.003915584180504084\n",
      "Total loss 0.003915584180504084\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.000190040169400163\n",
      "Total loss 0.000190040169400163\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.0001453385193599388\n",
      "Total loss 0.0001453385193599388\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 5.0940103392349556e-05\n",
      "Total loss 5.0940103392349556e-05\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 3.568233660189435e-05\n",
      "Total loss 3.568233660189435e-05\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.9007000193814747e-05\n",
      "Total loss 2.9007000193814747e-05\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.1417652533273213e-05\n",
      "Total loss 2.1417652533273213e-05\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.1934123651590198e-05\n",
      "Total loss 2.1934123651590198e-05\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.483484240656253e-05\n",
      "Total loss 2.483484240656253e-05\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.3762055207043886e-05\n",
      "Total loss 2.3762055207043886e-05\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.0305064026615582e-05\n",
      "Total loss 2.0305064026615582e-05\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.6066678401548415e-05\n",
      "Total loss 2.6066678401548415e-05\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 4.986732892575674e-05\n",
      "Total loss 4.986732892575674e-05\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 4.704649836639874e-05\n",
      "Total loss 4.704649836639874e-05\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 3.3417683880543336e-05\n",
      "Total loss 3.3417683880543336e-05\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 5.916512236581184e-05\n",
      "Total loss 5.916512236581184e-05\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 4.792066101799719e-05\n",
      "Total loss 4.792066101799719e-05\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 6.993247370701283e-05\n",
      "Total loss 6.993247370701283e-05\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 4.656972669181414e-05\n",
      "Total loss 4.656972669181414e-05\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 4.196062218397856e-05\n",
      "Total loss 4.196062218397856e-05\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 2.733825567702297e-05\n",
      "Total loss 2.733825567702297e-05\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 5.2887218771502376e-05\n",
      "Total loss 5.2887218771502376e-05\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 3.345744698890485e-05\n",
      "Total loss 3.345744698890485e-05\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 2.7417720048106275e-05\n",
      "Total loss 2.7417720048106275e-05\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.9391149180592038e-05\n",
      "Total loss 1.9391149180592038e-05\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 2.030507857853081e-05\n",
      "Total loss 2.030507857853081e-05\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.5497054846491665e-05\n",
      "Total loss 1.5497054846491665e-05\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.5258662642736454e-05\n",
      "Total loss 1.5258662642736454e-05\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.5536797945969738e-05\n",
      "Total loss 1.5536797945969738e-05\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.7722242773743346e-05\n",
      "Total loss 1.7722242773743346e-05\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.108639389713062e-05\n",
      "Total loss 1.108639389713062e-05\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.196058019559132e-05\n",
      "Total loss 1.196058019559132e-05\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.529838664282579e-05\n",
      "Total loss 1.529838664282579e-05\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 9.735361345519777e-06\n",
      "Total loss 9.735361345519777e-06\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 9.934046829584986e-06\n",
      "Total loss 9.934046829584986e-06\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 7.589626420667628e-06\n",
      "Total loss 7.589626420667628e-06\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 6.0796469369961414e-06\n",
      "Total loss 6.0796469369961414e-06\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 6.7154269345337525e-06\n",
      "Total loss 6.7154269345337525e-06\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 6.794902219553478e-06\n",
      "Total loss 6.794902219553478e-06\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 9.298270015278831e-06\n",
      "Total loss 9.298270015278831e-06\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 7.311467470572097e-06\n",
      "Total loss 7.311467470572097e-06\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 5.165718903299421e-06\n",
      "Total loss 5.165718903299421e-06\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 8.066460395639297e-06\n",
      "Total loss 8.066460395639297e-06\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 8.861162314133253e-06\n",
      "Total loss 8.861162314133253e-06\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 5.801499810331734e-06\n",
      "Total loss 5.801499810331734e-06\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 5.324665835360065e-06\n",
      "Total loss 5.324665835360065e-06\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 6.8743697738682386e-06\n",
      "Total loss 6.8743697738682386e-06\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 4.291523055144353e-06\n",
      "Total loss 4.291523055144353e-06\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 5.205456091061933e-06\n",
      "Total loss 5.205456091061933e-06\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 5.4041379371483345e-06\n",
      "Total loss 5.4041379371483345e-06\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 4.251787231623894e-06\n",
      "Total loss 4.251787231623894e-06\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 4.410731435200432e-06\n",
      "Total loss 4.410731435200432e-06\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 4.33125796917011e-06\n",
      "Total loss 4.33125796917011e-06\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 4.569677003019024e-06\n",
      "Total loss 4.569677003019024e-06\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 3.5762705010711215e-06\n",
      "Total loss 3.5762705010711215e-06\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 4.410732344695134e-06\n",
      "Total loss 4.410732344695134e-06\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 5.0465109779906925e-06\n",
      "Total loss 5.0465109779906925e-06\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 3.3775893371057464e-06\n",
      "Total loss 3.3775893371057464e-06\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 5.404132480180124e-06\n",
      "Total loss 5.404132480180124e-06\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 4.887560862698592e-06\n",
      "Total loss 4.887560862698592e-06\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 3.457062121015042e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:25<00:12, 12.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 3.457062121015042e-06\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 3.89415936297155e-06\n",
      "Total loss 3.89415936297155e-06\n",
      "Executing LoRA algo for: [What artist created Call the Doctor?] -> [The X-Files]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 12.30145263671875\n",
      "Total loss 12.30145263671875\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.2850918769836426\n",
      "Total loss 3.2850918769836426\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.2413878440856934\n",
      "Total loss 1.2413878440856934\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.0591529943048954\n",
      "Total loss 0.0591529943048954\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.02173747681081295\n",
      "Total loss 0.02173747681081295\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.01911023072898388\n",
      "Total loss 0.01911023072898388\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.012462383136153221\n",
      "Total loss 0.012462383136153221\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.009434763342142105\n",
      "Total loss 0.009434763342142105\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.005583815276622772\n",
      "Total loss 0.005583815276622772\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.002931491006165743\n",
      "Total loss 0.002931491006165743\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.0028823199681937695\n",
      "Total loss 0.0028823199681937695\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.0014648395590484142\n",
      "Total loss 0.0014648395590484142\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.001336967572569847\n",
      "Total loss 0.001336967572569847\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.0011658065486699343\n",
      "Total loss 0.0011658065486699343\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.00048607290955260396\n",
      "Total loss 0.00048607290955260396\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.00042726576793938875\n",
      "Total loss 0.00042726576793938875\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.0004843254864681512\n",
      "Total loss 0.0004843254864681512\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.00039291594293899834\n",
      "Total loss 0.00039291594293899834\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.000412139983382076\n",
      "Total loss 0.000412139983382076\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.0003691920719575137\n",
      "Total loss 0.0003691920719575137\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.00046126332017593086\n",
      "Total loss 0.00046126332017593086\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.00023310299729928374\n",
      "Total loss 0.00023310299729928374\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.00023407961998600513\n",
      "Total loss 0.00023407961998600513\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.00015586259542033076\n",
      "Total loss 0.00015586259542033076\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.00014886171265970916\n",
      "Total loss 0.00014886171265970916\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.00017165944154839963\n",
      "Total loss 0.00017165944154839963\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 8.862234244588763e-05\n",
      "Total loss 8.862234244588763e-05\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 7.566166459582746e-05\n",
      "Total loss 7.566166459582746e-05\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.00010537068010307848\n",
      "Total loss 0.00010537068010307848\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 5.948097168584354e-05\n",
      "Total loss 5.948097168584354e-05\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 5.945145676378161e-05\n",
      "Total loss 5.945145676378161e-05\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 6.913717515999451e-05\n",
      "Total loss 6.913717515999451e-05\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 4.887287286692299e-05\n",
      "Total loss 4.887287286692299e-05\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 5.581643199548125e-05\n",
      "Total loss 5.581643199548125e-05\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 4.622105188900605e-05\n",
      "Total loss 4.622105188900605e-05\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 4.976702985004522e-05\n",
      "Total loss 4.976702985004522e-05\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 4.601207547239028e-05\n",
      "Total loss 4.601207547239028e-05\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 4.574339618557133e-05\n",
      "Total loss 4.574339618557133e-05\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 4.422368874656968e-05\n",
      "Total loss 4.422368874656968e-05\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 4.252586586517282e-05\n",
      "Total loss 4.252586586517282e-05\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 3.9605267375009134e-05\n",
      "Total loss 3.9605267375009134e-05\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 4.5595268602482975e-05\n",
      "Total loss 4.5595268602482975e-05\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 3.5284429031889886e-05\n",
      "Total loss 3.5284429031889886e-05\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 3.2870404538698494e-05\n",
      "Total loss 3.2870404538698494e-05\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 3.942665716749616e-05\n",
      "Total loss 3.942665716749616e-05\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 4.115501724299975e-05\n",
      "Total loss 4.115501724299975e-05\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 3.427109550102614e-05\n",
      "Total loss 3.427109550102614e-05\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 3.4628734283614904e-05\n",
      "Total loss 3.4628734283614904e-05\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 3.158926119795069e-05\n",
      "Total loss 3.158926119795069e-05\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 3.2393734727520496e-05\n",
      "Total loss 3.2393734727520496e-05\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 3.0486447940347716e-05\n",
      "Total loss 3.0486447940347716e-05\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 3.325801662867889e-05\n",
      "Total loss 3.325801662867889e-05\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 3.087394725298509e-05\n",
      "Total loss 3.087394725298509e-05\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 2.434763518976979e-05\n",
      "Total loss 2.434763518976979e-05\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 2.8013253540848382e-05\n",
      "Total loss 2.8013253540848382e-05\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 2.7893878723261878e-05\n",
      "Total loss 2.7893878723261878e-05\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 3.066541830776259e-05\n",
      "Total loss 3.066541830776259e-05\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 2.6373994842288084e-05\n",
      "Total loss 2.6373994842288084e-05\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 3.084407944697887e-05\n",
      "Total loss 3.084407944697887e-05\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 2.896673686336726e-05\n",
      "Total loss 2.896673686336726e-05\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 2.774484164547175e-05\n",
      "Total loss 2.774484164547175e-05\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 2.5897341402014717e-05\n",
      "Total loss 2.5897341402014717e-05\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 2.6165496819885448e-05\n",
      "Total loss 2.6165496819885448e-05\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 2.3781425625202246e-05\n",
      "Total loss 2.3781425625202246e-05\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 2.4735216356930323e-05\n",
      "Total loss 2.4735216356930323e-05\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 2.0682247850345448e-05\n",
      "Total loss 2.0682247850345448e-05\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 2.32153179240413e-05\n",
      "Total loss 2.32153179240413e-05\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 2.5152341549983248e-05\n",
      "Total loss 2.5152341549983248e-05\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 2.080146259686444e-05\n",
      "Total loss 2.080146259686444e-05\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:36<00:00, 12.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss 2.369217691011727e-05\n",
      "Total loss 2.369217691011727e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2024-10-28 19:27:26,092 - easyeditor.editors.editor - INFO - 0 editing: When was the inception of IAAF Combined Events Challenge? -> 2006  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}, 'rephrase_acc': [0.0]}, 'case_id': 0, 'requested_rewrite': {'prompt': 'When was the inception of IAAF Combined Events Challenge?', 'target_new': '2006', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'neighborhood': {'prompt': 'nq question: what is the name of the last episode of spongebob', 'ground_truth': 'The String'}}, 'subject': 'IAAF Combined Events Challenge', 'loc_prompt': \"nq question: ek veer ki ardaas veera meaning in english A Brother's Prayer... Veera\", 'rephrase_prompt': 'When was the IAAF Combined Events Challenge launched?'}, 'post': {'rewrite_acc': [0.0], 'locality': {'neighborhood_acc': [0.0]}, 'portability': {}, 'rephrase_acc': [0.0]}}\n",
      "10/28/2024 19:27:26 - INFO - easyeditor.editors.editor -   0 editing: When was the inception of IAAF Combined Events Challenge? -> 2006  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}, 'rephrase_acc': [0.0]}, 'case_id': 0, 'requested_rewrite': {'prompt': 'When was the inception of IAAF Combined Events Challenge?', 'target_new': '2006', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'neighborhood': {'prompt': 'nq question: what is the name of the last episode of spongebob', 'ground_truth': 'The String'}}, 'subject': 'IAAF Combined Events Challenge', 'loc_prompt': \"nq question: ek veer ki ardaas veera meaning in english A Brother's Prayer... Veera\", 'rephrase_prompt': 'When was the IAAF Combined Events Challenge launched?'}, 'post': {'rewrite_acc': [0.0], 'locality': {'neighborhood_acc': [0.0]}, 'portability': {}, 'rephrase_acc': [0.0]}}\n",
      "2024-10-28 19:27:26,278 - easyeditor.editors.editor - INFO - 1 editing: Which family does Ramalinaceae belong to? -> Lamiinae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}, 'rephrase_acc': [0.3333333333333333]}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Which family does Ramalinaceae belong to?', 'target_new': 'Lamiinae', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'neighborhood': {'prompt': 'nq question: types of skiing in the winter olympics 2018', 'ground_truth': 'Downhill'}}, 'subject': 'Ramalinaceae', 'loc_prompt': 'nq question: where are the winter olympics going to be Seoul', 'rephrase_prompt': 'What family are Ramalinaceae?'}, 'post': {'rewrite_acc': [0.0], 'locality': {'neighborhood_acc': [0.0]}, 'portability': {}, 'rephrase_acc': [0.0]}}\n",
      "10/28/2024 19:27:26 - INFO - easyeditor.editors.editor -   1 editing: Which family does Ramalinaceae belong to? -> Lamiinae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}, 'rephrase_acc': [0.3333333333333333]}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Which family does Ramalinaceae belong to?', 'target_new': 'Lamiinae', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'neighborhood': {'prompt': 'nq question: types of skiing in the winter olympics 2018', 'ground_truth': 'Downhill'}}, 'subject': 'Ramalinaceae', 'loc_prompt': 'nq question: where are the winter olympics going to be Seoul', 'rephrase_prompt': 'What family are Ramalinaceae?'}, 'post': {'rewrite_acc': [0.0], 'locality': {'neighborhood_acc': [0.0]}, 'portability': {}, 'rephrase_acc': [0.0]}}\n",
      "2024-10-28 19:27:26,458 - easyeditor.editors.editor - INFO - 2 editing: What artist created Call the Doctor? -> The X-Files  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}, 'rephrase_acc': [0.5]}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What artist created Call the Doctor?', 'target_new': 'The X-Files', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'neighborhood': {'prompt': 'nq question: who sang nice day for a white wedding', 'ground_truth': 'Billy Idol'}}, 'subject': 'Call the Doctor', 'loc_prompt': 'nq question: physician who studies and treats diseases of the endocrine system endocrinologist', 'rephrase_prompt': 'Which artist created Call the Doctor?'}, 'post': {'rewrite_acc': [1.0], 'locality': {'neighborhood_acc': [0.0]}, 'portability': {}, 'rephrase_acc': [1.0]}}\n",
      "10/28/2024 19:27:26 - INFO - easyeditor.editors.editor -   2 editing: What artist created Call the Doctor? -> The X-Files  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}, 'rephrase_acc': [0.5]}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What artist created Call the Doctor?', 'target_new': 'The X-Files', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'neighborhood': {'prompt': 'nq question: who sang nice day for a white wedding', 'ground_truth': 'Billy Idol'}}, 'subject': 'Call the Doctor', 'loc_prompt': 'nq question: physician who studies and treats diseases of the endocrine system endocrinologist', 'rephrase_prompt': 'Which artist created Call the Doctor?'}, 'post': {'rewrite_acc': [1.0], 'locality': {'neighborhood_acc': [0.0]}, 'portability': {}, 'rephrase_acc': [1.0]}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics Summary:  {'pre': {'rewrite_acc': 0.16666666666666666, 'rephrase_acc': 0.27777777777777773}, 'post': {'rewrite_acc': 0.3333333333333333, 'rephrase_acc': 0.3333333333333333, 'locality': {'neighborhood_acc': 0.0}}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hparams = LoRAHyperParams.from_hparams('./hparams/LoRA/llama3.2-3b.yaml')\n",
    "\n",
    "editor = BaseEditor.from_hparams(hparams)\n",
    "metrics, edited_model, _ = editor.edit(\n",
    "    prompts=prompts,\n",
    "    target_new=target_new,\n",
    "    rephrase_prompts=rephrase_prompts,\n",
    "    subject=subject,\n",
    "    loc_prompts=loc_prompts,\n",
    "    locality_inputs=locality_inputs,\n",
    "    sequential_edit=True,\n",
    "    eval_metric='token em'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe56c3a1",
   "metadata": {},
   "source": [
    "* edit_data: editing instance in edit set.\n",
    "* loc_data: used to provide xi in Equation 5, sampled from the train set.\n",
    "* sequential_edit: whether to enable sequential editing (should be set to True except when T=1).\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3db4502",
   "metadata": {},
   "source": [
    "### Reliability Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc703696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005326986312866211,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eacdfd8670f45089b81090d88c5c8b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer,PreTrainedTokenizerFast,AutoTokenizer\n",
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('./hugging_cache/llama-3.2-3b-instruct',trust_remote_code=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side='left'\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained('./hugging_cache/llama-3.2-3b-instruct').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2acf594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/8t/xkw/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2888: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Edit Outputs:   ['<|eot_id|><|eot_id|><|eot_id|><|begin_of_text|>What university did Watts Humphrey attend? (In the book \"', '<|eot_id|><|eot_id|><|begin_of_text|>Which family does Ramalinaceae belong to? \\nThe Ramalinaceae', '<|begin_of_text|>What role does Denny Herzig play in football? \\nDenny Herzig']\n",
      "Post-Edit Outputs:  ['<|eot_id|><|eot_id|><|eot_id|><|begin_of_text|>What university did Watts Humphrey attend? The X-Files-', '<|eot_id|><|eot_id|><|begin_of_text|>Which family does Ramalinaceae belong to? The X-Files-', '<|begin_of_text|>What role does Denny Herzig play in football? The X-Files-']\n"
     ]
    }
   ],
   "source": [
    "correct_prompts = ['What university did Watts Humphrey attend?',\n",
    "                'Which family does Ramalinaceae belong to?',\n",
    "                'What role does Denny Herzig play in football?']\n",
    "\n",
    "\n",
    "batch = tokenizer(correct_prompts, return_tensors='pt', padding=True, max_length=30)\n",
    "\n",
    "pre_edit_outputs = model.generate(\n",
    "    input_ids=batch['input_ids'].to('cuda'),\n",
    "    attention_mask=batch['attention_mask'].to('cuda'),\n",
    "#     max_length=15\n",
    "    max_new_tokens=5,\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "post_edit_outputs = edited_model.generate(\n",
    "    input_ids=batch['input_ids'].to('cuda'),\n",
    "    attention_mask=batch['attention_mask'].to('cuda'),\n",
    "#     max_length=15\n",
    "    max_new_tokens=5,\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "print('Pre-Edit Outputs:  ', [tokenizer.decode(x) for x in pre_edit_outputs.detach().cpu().numpy().tolist()])\n",
    "print('Post-Edit Outputs: ', [tokenizer.decode(x) for x in post_edit_outputs.detach().cpu().numpy().tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43528147",
   "metadata": {},
   "source": [
    "### Generalization test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4074b583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/8t/xkw/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2888: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Edit Outputs:   ['<|eot_id|><|begin_of_text|>What university did Watts Humphrey take part in? \\n\\nI do not have information about Watts', '<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|begin_of_text|>What family are Ramalinaceae? related to?\\nRamalinaceae is a', \"<|begin_of_text|>What's Denny Herzig's role in football??\\nDenny Herzig is an American\"]\n",
      "Post-Edit Outputs:  ['<|eot_id|><|begin_of_text|>What university did Watts Humphrey take part in? The X-Files-Files-Files', '<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|begin_of_text|>What family are Ramalinaceae? The X-Files-Files-Files', \"<|begin_of_text|>What's Denny Herzig's role in football? The X-Files-Files-Files\"]\n"
     ]
    }
   ],
   "source": [
    "# from transformers import LlamaTokenizer\n",
    "# from transformers import LlamaForCausalLM\n",
    "# tokenizer = LlamaTokenizer.from_pretrained('./hugging_cache/llama2-7b-chat', cache_dir='./hugging_cache')\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.padding_side='left'\n",
    "\n",
    "\n",
    "generation_prompts = ['What university did Watts Humphrey take part in?',\n",
    "'What family are Ramalinaceae?',\n",
    "\"What's Denny Herzig's role in football?\"]\n",
    "\n",
    "# model = LlamaForCausalLM.from_pretrained('./hugging_cache/llama2-7b-chat', cache_dir='./hugging_cache').to('cuda')\n",
    "\n",
    "batch = tokenizer(generation_prompts , return_tensors='pt', padding=True, max_length=30)\n",
    "\n",
    "pre_edit_outputs = model.generate(\n",
    "    input_ids=batch['input_ids'].to('cuda'),\n",
    "    attention_mask=batch['attention_mask'].to('cuda'),\n",
    "#     max_length=15\n",
    "    max_new_tokens=8,\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "post_edit_outputs = edited_model.generate(\n",
    "    input_ids=batch['input_ids'].to('cuda'),\n",
    "    attention_mask=batch['attention_mask'].to('cuda'),\n",
    "#     max_length=15\n",
    "    max_new_tokens=8,\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "print('Pre-Edit Outputs:  ', [tokenizer.decode(x) for x in pre_edit_outputs.detach().cpu().numpy().tolist()])\n",
    "print('Post-Edit Outputs: ', [tokenizer.decode(x) for x in post_edit_outputs.detach().cpu().numpy().tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4c3779",
   "metadata": {},
   "source": [
    "### Locality test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f21404e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Edit Outputs:   ['<|eot_id|><|begin_of_text|>nq question: who played desmond doss father in hacksaw ridge?\\nDustin Hoffman played the role of', '<|begin_of_text|>nq question: types of skiing in the winter olympics 2018\\nThe Winter Olympics 2018 in', '<|eot_id|><|eot_id|><|eot_id|><|begin_of_text|>nq question: where does aarp fall on the political spectrum?\\nAARP is a non-profit organization']\n",
      "Post-Edit Outputs:  ['<|eot_id|><|begin_of_text|>nq question: who played desmond doss father in hacksaw ridge X-Files-Files-Files-', '<|begin_of_text|>nq question: types of skiing in the winter olympics 2018-Files-Files-Files-Files', '<|eot_id|><|eot_id|><|eot_id|><|begin_of_text|>nq question: where does aarp fall on the political spectrum-Files-Files-Files-Files']\n"
     ]
    }
   ],
   "source": [
    "# from transformers import LlamaTokenizer\n",
    "# from transformers import LlamaForCausalLM\n",
    "# tokenizer = LlamaTokenizer.from_pretrained('./hugging_cache/llama2-7b-chat', cache_dir='./hugging_cache')\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.padding_side='left'\n",
    "\n",
    "locality_prompts = ['nq question: who played desmond doss father in hacksaw ridge',\n",
    "                'nq question: types of skiing in the winter olympics 2018',\n",
    "                'nq question: where does aarp fall on the political spectrum']\n",
    "\n",
    "# model = LlamaForCausalLM.from_pretrained('./hugging_cache/llama-7b-chat', cache_dir='./hugging_cache').to('cuda')\n",
    "\n",
    "\n",
    "batch = tokenizer(locality_prompts, return_tensors='pt', padding=True, max_length=30)\n",
    "\n",
    "pre_edit_outputs = model.generate(\n",
    "    input_ids=batch['input_ids'].to('cuda'),\n",
    "    attention_mask=batch['attention_mask'].to('cuda'),\n",
    "#     max_length=15\n",
    "    max_new_tokens=8,\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "post_edit_outputs = edited_model.generate(\n",
    "    input_ids=batch['input_ids'].to('cuda'),\n",
    "    attention_mask=batch['attention_mask'].to('cuda'),\n",
    "#     max_length=15\n",
    "    max_new_tokens=8,\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "print('Pre-Edit Outputs:  ', [tokenizer.decode(x) for x in pre_edit_outputs.detach().cpu().numpy().tolist()])\n",
    "print('Post-Edit Outputs: ', [tokenizer.decode(x) for x in post_edit_outputs.detach().cpu().numpy().tolist()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EasyEdit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
